
{
    "cells": [
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "de_train = pd.read_csv(\"./train_dev_test_splits/de.train.csv\", sep=\"\\t\")\n",
       "de_val = pd.read_csv(\"./train_dev_test_splits/de.valid.csv\", sep=\"\\t\")\n",
       "de_test = pd.read_csv(\"./train_dev_test_splits/de.test.csv\", sep=\"\\t\")\n",
       "\n",
       "fr_train = pd.read_csv(\"./train_dev_test_splits/fr.train.csv\", sep=\"\\t\")\n",
       "fr_val = pd.read_csv(\"./train_dev_test_splits/fr.valid.csv\", sep=\"\\t\")\n",
       "fr_test = pd.read_csv(\"./train_dev_test_splits/fr.test.csv\", sep=\"\\t\")\n"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Spacy Pipeline - Tokenization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "import spacy\n",
       "\n",
       "nlp_de = spacy.load(\"de_core_news_sm\")\n",
       "nlp_fr = spacy.load(\"fr_core_news_sm\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
       "import regex\n",
       "import emoji\n",
       "from nltk.corpus import stopwords\n",
       "\n",
       "def processor(data, lang, nlp):\n",
       "    reg = '[^a-zA-Z0-9 àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]' if lang == 'fr' else '[^a-zA-Z0-9 äöüßÄÖÜẞ]'\n",
       "    sw = set(stopwords.words('french' if lang=='fr' else 'german'))\n",
       "    corpus = []\n",
       "    for i in range(0, data['content'].size):\n",
       "        text = regex.sub(r'<U\\+([0-9a-fA-F]+)>', lambda m: chr(int(m.group(1),16)), data['content'][i])\n",
       "        text = emoji.demojize(text, language=lang)\n",
       "        # get review and remove non alpha chars\n",
       "        text = regex.sub(reg, ' ', text)\n",
       "        text = text.lower()\n",
       "        # split into tokens, apply stemming and remove stop words\n",
       "        text = ' '.join([t.text for t in nlp(text)])\n",
       "        corpus.append(text)\n",
       "\n",
       "    return corpus\n",
       "\n",
       "\n"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## TF-IDF Vectorizer"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "(3678, 16343)\n",
         "(3678,)\n"
        ]
       }
      ],
      "source": [
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "\n",
       "corpus = processor(fr_train, 'fr', nlp_fr) + processor(fr_val, 'fr', nlp_fr) + processor(fr_test, 'fr', nlp_fr)\n",
       "vectorizer = TfidfVectorizer()\n",
       "X = vectorizer.fit_transform(corpus)\n",
       "print(X.shape)\n",
       "\n",
       "y = np.concatenate((fr_train[\"e1\"].values, fr_val[\"e1\"].values, fr_test[\"e1\"].values))\n",
       "print(y.shape)\n"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## French DataSet"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
       "from sklearn.model_selection import train_test_split\n",
       "\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
         "[[582   0   0]\n",
         " [ 35   0   2]\n",
         " [108   1   8]]\n",
         "0.8016304347826086\n"
        ]
       }
      ],
      "source": [
       "from sklearn.svm import SVC\n",
       "from sklearn.metrics import confusion_matrix, accuracy_score\n",
       "\n",
       "clf = SVC()\n",
       "clf.fit(X_train, y_train)\n",
       "y_pred = clf.predict(X_test)\n",
       "print(y_pred)\n",
       "print(confusion_matrix(y_test, y_pred))\n",
       "print(accuracy_score(y_test, y_pred))"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## German Dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "(4306, 22102)\n",
         "(4306,)\n"
        ]
       }
      ],
      "source": [
       "corpus = processor(de_train, 'de', nlp_de) + processor(de_val, 'de', nlp_de) + processor(de_test, 'de', nlp_de)\n",
       "vectorizer = TfidfVectorizer()\n",
       "X = vectorizer.fit_transform(corpus)\n",
       "print(X.shape)\n",
       "\n",
       "y = np.concatenate((de_train[\"e1\"].values, de_val[\"e1\"].values, de_test[\"e1\"].values))\n",
       "print(y.shape)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
         " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
         " 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
         "[[582   0   0]\n",
         " [ 35   0   2]\n",
         " [108   1   8]]\n",
         "0.8016304347826086\n"
        ]
       }
      ],
      "source": [
       "clf = SVC()\n",
       "clf.fit(X_train, y_train)\n",
       "y_pred = clf.predict(X_test)\n",
       "print(y_pred)\n",
       "print(confusion_matrix(y_test, y_pred))\n",
       "print(accuracy_score(y_test, y_pred))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "\n",
       "def text_to_mean_vector(embeddings, text):\n",
       "    tokens = text.split()\n",
       "    \n",
       "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
       "    vec = []\n",
       "    i = 0\n",
       "    while i < len(tokens):   # while there are tokens and did not reach desired sequence length\n",
       "        try:\n",
       "            vec.append(embeddings.get_vector(tokens[i]))\n",
       "        except KeyError:\n",
       "            True   # simply ignore out-of-vocabulary tokens\n",
       "        finally:\n",
       "            i += 1\n",
       "    \n",
       "    # add blanks up to sequence_len, if needed\n",
       "    vec = np.mean(vec, axis=0)\n",
       "    return vec\n",
       "\n",
       "def text_to_vector(embeddings, text, sequence_len):\n",
       "    \n",
       "    # split text into tokens\n",
       "    tokens = text.split()\n",
       "    \n",
       "    # convert tokens to embedding vectors, up to sequence_len tokens\n",
       "    vec = []\n",
       "    n = 0\n",
       "    i = 0\n",
       "    while i < len(tokens) and n < sequence_len:   # while there are tokens and did not reach desired sequence length\n",
       "        try:\n",
       "            vec.extend(embeddings.get_vector(tokens[i]))\n",
       "            n += 1\n",
       "        except KeyError:\n",
       "            True   # simply ignore out-of-vocabulary tokens\n",
       "        finally:\n",
       "            i += 1\n",
       "    \n",
       "    # add blanks up to sequence_len, if needed\n",
       "    for j in range(sequence_len - n):\n",
       "        vec.extend(np.zeros(embeddings.vector_size,))\n",
       "    \n",
       "    return vec"
      ]
     },
     {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Embeddings pfv"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "1 296 58.20274036228518 43.038069559552135 ModeResult(mode=array([17]), count=array([71]))\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/tmp/ipykernel_10700/3424571105.py:6: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
         "  print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))\n"
        ]
       }
      ],
      "source": [
       "from scipy import stats\n",
       "\n",
       "corpus = processor(de_train, 'de', nlp_de) + processor(de_val, 'de', nlp_de) + processor(de_test, 'de', nlp_de)\n",
       "lens = [len(c.split()) for c in corpus]\n",
       "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "und schreiben ihren      deutschland wird islamischer   was verstehen sie darunter   ja   eine multikulturelle gesellschaft beherbergt menschen   unterschiedlicher religionen und farben und unter ihnen wird es konservative und liberale gläubige geben   was hat das mit dem eigentlichen thema zu tun  \n"
        ]
       }
      ],
      "source": [
       "print(corpus[0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "1 642 41.43637846655791 45.65519818612259 ModeResult(mode=array([16]), count=array([98]))\n"
        ]
       },
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/tmp/ipykernel_10700/2042929227.py:3: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
         "  print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))\n"
        ]
       }
      ],
      "source": [
       "corpus_fr = processor(fr_train, 'fr', nlp_fr) + processor(fr_val, 'fr', nlp_fr) + processor(fr_test, 'fr', nlp_fr)\n",
       "lens = [len(c.split()) for c in corpus_fr]\n",
       "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
     },
     "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }